library(twitteR)
library(sentiment)
library(plyr)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
viki.tweets = searchTwitteR("viki", n=1500, lang="en")
api_key ="NnsMrGv2CEF8g71LqVOiHdXeg"
api_secret ="OCt39dwLhpt9WFKRq3mD9k4o2zSRagLr3GZVBjBAFzAX09pe5I"
access_token ="142569552-AHcBcvwBckHbQOiWiwq99iXErWHJlQ7QRphyTHqz"
access_token_secret ="mftLRi6SDv0fJ09cePgw4ae5rAJNgY3KB7rW8K68mWXa0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
viki.tweets = searchTwitteR("viki", n=1500, lang="en")
tweet = viki.tweets[[1]]
class(tweet)
tweet$getScreenName()
tweet$getText()
tweet_text = sapply(viki.tweets, function(x) x$getText())
length(twee_text)
length(tweet_text)
head(tweet_text, 100)
#most used words
tweets = userTimeline("Viki", n = 3200)
n.tweet = length(tweets)
## convert tweets to a data frame
tweets.df <- twListToDF(tweets)
dim(tweets.df)
for (i in c(1:2, 320)) {
cat(paste0("[", i, "] "))
writeLines(strwrap(tweets.df$text[i], 60))
}
library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
## remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")
# remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
myCorpus <- tm_map(myCorpus, stemDocument)
library(SnowballC)
install.packages("SnowballC")
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect the first 5 documents (tweets)
inspect(myCorpus[1:5])
# The code below is used for to make text fit for paper width
for (i in c(1:2, 320)) {
cat(paste0("[", i, "] "))
writeLines(strwrap(as.character(myCorpus[[i]]), 60))
}
## [1] exampl call java code r
## [2] simul mapreduc r big data analysi use flight data rblogger
## [320] r refer card data mine now cran list mani use r function
## packag data mine applic
# tm v0.5-10
#myCorpus <- tm_map(myCorpus, stemCompletion)
# tm v0.6
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
# Unexpectedly, stemCompletion completes an empty string to
# a word in dictionary. Remove empty string to avoid above issue.
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
miningCases <- lapply(myCorpusCopy,
function(x) { grep(as.character(x), pattern = "\\<mining")} )
sum(unlist(miningCases))
minerCases <- lapply(myCorpusCopy,
function(x) {grep(as.character(x), pattern = "\\<miner")} )
sum(unlist(minerCases))
miningCases <- lapply(myCorpusCopy,
function(x) { grep(as.character(x), pattern = "\\<viki")} )
sum(unlist(miningCases))
minerCases <- lapply(myCorpusCopy,
function(x) {grep(as.character(x), pattern = "\\<korea")} )
sum(unlist(minerCases))
myCorpus <- tm_map(myCorpus, content_transformer(gsub),
pattern = "korea", replacement = "viki")
tdm <- TermDocumentMatrix(myCorpus,
tdm
control = list(wordLengths = c(1, Inf)))
idx <- which(dimnames(tdm)$Terms == "r")
tdm <- TermDocumentMatrix(myCorpus,
tdm
## <<TermDocumentMatrix (terms: 822, documents: 320)>>
## Non-/sparse entries: 2460/260580
## Sparsity : 99%
## Maximal term length: 27
## Weighting : term frequency (tf)
control = list(wordLengths = c(1, Inf)))
tdm <- TermDocumentMatrix(myCorpus,
control = list(wordLengths = c(1, Inf)))
idx <- which(dimnames(tdm)$Terms == "r")
inspect(tdm[idx + (0:5), 101:110])
idx <- which(dimnames(tdm)$Terms == "Korean drama")
inspect(tdm[idx + (0:5), 101:110])
idx <- which(dimnames(tdm)$Terms == "Kdrama")
inspect(tdm[idx + (0:5), 101:110])
idx <- which(dimnames(tdm)$Terms == "viki")
inspect(tdm[idx + (0:5), 101:110])
(freq.terms <- findFreqTerms(tdm, lowfreq = 15))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 15)
df <- data.frame(term = names(term.freq), freq = term.freq)
library(ggplot2)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
findAssocs(tdm, "viki", 0.2)
findAssocs(tdm, "korea", 0.25)
findAssocs(tdm, "Korea", 0.25)
findAssocs(tdm, "Kdrama", 0.25)
findAssocs(tdm, "Kpop", 0.25)
findAssocs(tdm, "leeminho", 0.25)
findAssocs(tdm, "kimsoohyeon", 0.25)
findAssocs(tdm, "kdrama", 0.25)
findAssocs(tdm, "romance", 0.25)
findAssocs(tdm, "viki", 0.25)
library(graph)
findAssocs(tdm, "viki", 0.25)
library(graph)
install.packages("graph")
library(Rgraphviz)
plot(tdm, term = freq.terms, corThreshold = 0.1, weighting = T)
install.packages("Rgraphviz")
findAssocs(tdm, "viki", 0.2)
# which words are associated with 'mining'?
findAssocs(tdm, "viki", 0.25)
install.packages("graph")
require(devtools)
install_url("http://cran.r-project.org/src/contrib/Archive/graph/graph_1.30.0.tar.gz")
library(BiocInstaller)
install.packages("BiocInstaller")
source(“http://bioconductor.org/biocLite.R”)
install_url("http://cran.r-project.org/src/contrib/Archive/graph/graph_1.30.0.tar.gz")
source(“http://bioconductor.org/biocLite.R”)
source("https://bioconductor.org/biocLite.R")
biocLite("RBGL")
library(graph)
library(Rgraphviz)
install.packages("Rgraphviz")
source("https://bioconductor.org/biocLite.R")
biocLite("Rgraphviz")
library(Rgraphviz)
plot(tdm, term = freq.terms, corThreshold = 0.1, weighting = T)
m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
# colors
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:4)]
# plot word cloud
library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3,
random.order = F, colors = pal)
warning()
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
viki.tweets = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", viki.tweets)
viki.tweets = gsub("@\\w+", "", viki.tweets)
viki.tweets = gsub("[[:punct:]]", "", viki.tweets)
viki.tweets = gsub("[[:digit:]]", "", viki.tweets)
viki.tweets = gsub("http\\w+", "", viki.tweets)
viki.tweets = gsub("[ \t]{2,}", "", viki.tweets)
viki.tweets = gsub("^\\s+|\\s+$", "", viki.tweets)
try.error = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
viki.tweets = sapply(viki.tweets, try.error)
viki.tweets = viki.tweets[!is.na(viki.tweets)]
names(viki.tweets) = NULL
class_emo = classify_emotion(viki.tweets, algorithm="bayes", prior=1.0)
emotion = class_emo[,7]
emotion[is.na(emotion)] = "unknown"
class_pol = classify_polarity(viki.tweets, algorithm="bayes")
polarity = class_pol[,4]
sent_df = data.frame(text=viki.tweets, emotion=emotion,
polarity=polarity, stringsAsFactors=FALSE)
sent_df = within(sent_df,
emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
ggplot(sent_df, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette="Dark2") +
labs(x="emotion categories", y="number of tweets")
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of tweets")
emos = levels(factor(sent_df$emotion))
nemo = length(emos)
emo.docs = rep("", nemo)
for (i in 1:nemo)
{
tmp = viki.tweets[emotion == emos[i]]
emo.docs[i] = paste(tmp, collapse=" ")
}
emo.docs = removeWords(emo.docs, stopwords("english"))
corpus = Corpus(VectorSource(emo.docs))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = emos
comparison.cloud(tdm, colors = brewer.pal(nemo, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
library(sentiment)
library(plyr)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
#sentitment
viki.tweets = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", viki.tweets)
viki.tweets = gsub("@\\w+", "", viki.tweets)
viki.tweets = gsub("[[:punct:]]", "", viki.tweets)
viki.tweets = gsub("[[:digit:]]", "", viki.tweets)
viki.tweets = gsub("http\\w+", "", viki.tweets)
viki.tweets = gsub("[ \t]{2,}", "", viki.tweets)
viki.tweets = gsub("^\\s+|\\s+$", "", viki.tweets)
try.error = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
viki.tweets = sapply(viki.tweets, try.error)
viki.tweets = viki.tweets[!is.na(viki.tweets)]
names(viki.tweets) = NULL
class_emo = classify_emotion(viki.tweets, algorithm="bayes", prior=1.0)
emotion = class_emo[,7]
emotion[is.na(emotion)] = "unknown"
class_pol = classify_polarity(viki.tweets, algorithm="bayes")
polarity = class_pol[,4]
sent_df = data.frame(text=viki.tweets, emotion=emotion,
polarity=polarity, stringsAsFactors=FALSE)
sent_df = within(sent_df,
emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
ggplot(sent_df, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette="Dark2") +
labs(x="emotion categories", y="number of tweets")
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of tweets")
source('D:/김진주/PhD/Github/testingTextMiningTools/Testing_twitter.R')
1:2
1:20
c(1:2, 32)
c(1:2, 320)
head(class_emo)
class_emo = classify_emotion(viki.tweets$text, algorithm="bayes", prior=1.0)
class_emo
viki.tweets$text
